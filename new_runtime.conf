system:
  daemon-port: 7001
  daemon-host: 0.0.0.0
  generation: gen2
  # Performance estimation mode. When set to true, the system will store traces
  # of the running application for analysis purposes.
  # Only supported for `generation: gen2`.
  enable-performance-estimation: false

daemon:
  # debug: false
  elrond:
    path: elrond
    first-port: 7003

  # The maximum number of devices to use in parallel.
  # Defaults (if not set) to the number of cards in the system, or 1 for nextengined.
  # elrond port will be reserved for each device starting from first-port.
  # To enable multi-card support, remove this entry.
  device-limit: 1
  simulator:
  - port: 7002
  # Enable eventlog: a trace for system events
  # enable-eventlog: true
  # Run the software simulator, even if the hardware is present.
  # force-software-only: false
  #
  # Set to false if the simulator should not be reset upon connection:
  # reset-on-connection: true
  #
  nextmonitor:
    # Set to true to start and close nextmonitor along with nextdaemon
    # auto-start: true
    # Provide the path to nextmonitor. Optional
    path: nextmonitor
    # Provide the path to the nextmonitor's DB file. Optional
    data-path: ./nextsilicon/data/nextmonitor.sqlite
    # Set the monitor's log-level. If not provided, would inherit the daemon's
    # log-level if specified in the command line, or would use the default level
    # otherwise
    # log-level: info
    # If true, enables the monitor's dynamic sample period algorithm, which
    #  changes the query period according to the time Elrond takes to reply
    # dynamic-sampling: true

  # Settings for the UI services.
  ui:
    # Whether to run the UI's collector service when starting the daemon.
    # Enabled by default, unless nextmonitor is disabled.
    enable: true
    # Host the collector service will listen on.
    host: 127.0.0.1
    # Port the collector service will listen on.
    collector-port: 7010
    # Path to the UI's artifacts directory.
    # Note: when running via the nsopt executable, the database path is overriden to `[nsopt_out_dir]/ui`.
    database-path: ./nextsilicon/data

execution:
  deploy-scheme:
    # Assign mill peripheral BBGs to host/risc (default: risc):
    mill-peripheral: default
    # Assign mill core BBGs to host/risc/grid (default: grid):
    mill-core: default

  # host-agent:
    # Number of worker threads in the Host-Agent's uEMU,
    # This will be used to emulate host bbgs only when RISC disabled.
    # 0 means disabled uEMU (grid support still enabled from host)
    # worker-threads: 0
    # Choose between using a software implementation of libcalls ("soft"),
    # or x86 hardware implementation ("hard").
    # The hardware provides better performance with a possible loss of accuracy.
    # default: full x86 hardware acceleration (full_hardware).
    # hardware_integer: only accelerate integer operations.
    # hardware_float: only accelerate floating point operations.
    # none: use software-implemented libcalls.
    # libcall-support: none | hardware_integer | hardware_float | full_hardware
    # drop-zero-ctr-tlm-packets: true

  # device-telemetry:
  #   Telemetry sample interval in milliseconds: (default: 1000)
  #   sample-interval: <value>
  #   nxt009-telemetry-dump-file: raw_telemetry_data.tlm
  #   Time beteween dumps to the telemetry dump file in miliseconds
  #   telem-dump-interval-ms: 20000
  #   enable-hbm-telemetry: true
  #   hbm-telemetry-quad-num: 4

  # Set to apply mill thread limits as configured by the optimizer. (default: true)
  # apply-thread-limits: <value>

  # Set to false to prepare the change objects but do not apply them onto the domains.
  # apply-change: true

  # If set to true, the selection of the duplication instance will be based on the affinity selection policy described below,
  # and this will cause the RISC memory exceptions to migrate based on thread affinity and future thread execution on grid.
  # Setting to false will fallback to unordered load balancing which lets each process use all the instances,
  # and won't cause migrations to be local to future thread execution on grid.
  # spatial-process-affinity: true

  # affinity-selection:
  # Build execution load balancing in a manner that preserves affinity to a spatial area on the grid and takes advantage of memory locality.
  # The distribution of threads across duplication instances can be based on either of the following:
  # (a) round-robin policy (default for processes), which interleaves between instances
  # (b) bulk policy (default for threads), where each instance executes a contiguous sub-range (requires enabling RISC OpenMP, see below),
  # (c) custom selection using an additional mask parameter, or -
  # (d) disabled, meaning all threads are executed as if they originated from one process,
  #     spread across instances according to the thread affinity policy.
  #   process-affinity-policy: round-robin | bulk | process-custom-balance-mask | disabled
  #   thread-affinity-policy: round-robin | bulk | thread-custom-balance-mask
  # Set a custom mask that will be applied to the process and thread ids, respectively, in order to select the duplication instance. int value (default: -1, any instance)
  # Example: setting process-custom-balance-mask=0x1f8000 means the spatial area on the grid will be selected using these 6 bits of each process ID,
  #          setting thread-custom-balance-mask=0x7fff means the instance within that area will be selected using the 15 bits of each thread ID.
  #   process-custom-balance-mask: <value>
  #   thread-custom-balance-mask: <value>

  # root-page-table-location:
  # Configuration for the location of the root page table on each quad.
  # Default configuration is to let linear allocator decide where to allocate.
  #   row: <value>
  #   col: <value>
  #   dcode: <value>
  #   force: <true|false>

  # Setting to true will force zeroing page tables on table allocation
  # zero-hbm-on-table-alloc: true

projection:
  # Path to silicon layout: in gen2 sl_q0-3_r0-7_c0-7_hbm3e_24GB.json, in gen3 sl_q0_r0_c7_no_bmt.json
  # silicon-layout: default
  nxt009:
 
    ################## Product-safe fields ##################
    # The following fields all behave well together, and are fully supported with runtime & HW.

    # Higher optimization level will take longer to compute. Available values: min, low, high, max.
    projection-optimization-level: high
    # projection-config-flavor: legacy

    # mill-duplications-per-region sets the duplication policy per region for all mills.
    # At least one duplication must be placed per mill.
    # - mill-duplications-per-region >= 1: requires each mill to have that duplication count in each region.
    # - mill-duplications-per-region = 0: projection will fill the grid with maximum duplications of
    #   all mills. Mills may have different duplication counts.
    mill-duplications-per-region: 0

    # CBUs in `prohibited-cbus` will be prohibited from having any projection configured.
    # Separate with a ':' character. CBU format: <die,quad,row,col>
    # Example: "0,0,2,3:0,0,2,4:0,0,2,5"
    # prohibited-cbus: ""

    # Optional. single-ib-window bounds the projected mill shape.
    # Projection will succeed only if each mill's shape for a single duplication can fit within the single-ib-window shape.
    single-ib-window: "4,7"

    # CG split scheme. For each mill, place every BBG in its own CG (single-bbg)
    # or place all of them into one CG (single-cg).
    # cg-split: single-cg

    # CG duplication policy.
    #   - disabled (default): No CG duplication will be applied.
    #   - naive: use a naive algorithm to determine the duplication count for each CG based on the mill structure.
    #   - optimized: use the performance model score to choose the best duplication count for each CG.
    # cg-duplication-policy: disabled

    # Projection Caching. Enabled by default, with a cache path and size as listed below.
    # The cache usese an LRU (Lease Recently Used) eviction policy.
    # cache-enabled: true
    cache-path: /space2/users/mathiasp/.cache/nextsilicon
    cache-max-size-mib: 1024  # Defaults to 1GiB

    ################## Experimental/research fields ##################
    # The following fields may be useful for research, but may have unexpected behaviour, especially
    # when running with runtime.
    # Warning: The user is responsible for checking compatibility with other fields, including
    # runtime fields.

    # Use linear mode.
    # meps-linear-mode-enabled: false
    # HBM linear base address.
    # meps-linear-base-address: 0x6000000000
    # <TODO: Missing documentation>.
    # ceps-ib-allocation-offset: 0
    # Force all MEPs to support high-throughput split transactions.
    # force-high-tp-meps: false
    # Use AGU for memory accesses when possible
    # lower_lea_to_agu: true
    # Projection window boundaries. The window is mutually exclusive with use-numa-regions: true.
    # The projection window is used for testing, and in all relevant ways is treated as a single region.
    # If the window is set to "SL", then the window is set according to the provided SL.
    # window: "<row>,<col>,<height>,<width>" or "SL"
    # Share the same IB over all IB domains.
    # share-ib-between-domains: false
    # Telemetry sampling interval.
    telemetry-sample-interval-ms: 10
    # Whether to sync the interval between all agents
    # If true, will cause all enabled agents to send a packet on the same timestamp
    # telemetry-sync-interval: false
    # The location of the telemetry event queue in the grid.
    # There are two location formats:
    # - hostif
    # - 'mcu:<die>,<quad>,<row>,<col>'
    # telemetry-eq: hostif
    # Comma-delimited list of telemetries to report. Available types:
    # load, iqr-num-threads, iqd-rsid-run-count, mcu-bin, mep-mmu-rma, memcip-mcu-hit-type, nbus-lnb, l1-dump, l2-dump, nbus-backpressure.
    enabled-telemetries: "load,mep-mmu-rma,memcip-mcu-hit-type"
    # The location of the exception event queue in the grid.
    # There are two location formats:
    # - hostif
    # - 'mcu:<die>,<quad>,<row>,<col>'
    # exception-eq: hostif
    # Comma-delimited list of exceptions to report. Available types:
    # cbu-mep-exceptions.
    # enabled-exceptions: cbu-mep-exceptions
    # Json file allowing the user to add telemetry requests
    # telem-cfg-file: <file full path>
    # Use optimizer.nxt009 NUMA region settings to control the projection
    # request. This feature is mutually exclusive with the projection window.
    # use-numa-regions: true
    # The maximum amount of user processes that can simultaneously have
    # handed-off thread(s) inside the NextSilicon runtime.
    # (0) indicates no configured limit.
    # NOTE: Having more than 1 in this field is experimental, do not use.
    # max-processes-per-region: 1
    # The maximum amount of runtime threads that can be used in a single NUMA region.
    # The default setting (0) indicates no configured limit; it will allow the
    # maximum number of addressable threads per region.
    # max-threads-per-region: 0
    # Control number of algorithm iterations and configurations to try, attempting to achieve better projection result.
    # If using multiple slots, do not error out on the first failure and finish projecting all subsequent slots
    # Default (false) indicates that by default the first failure will error out
    # continue-on-projection-error: false
    # If projection failed to converge while projecting a slot, attempt to project the rest of the contents even though
    # it would eventually fail. Projection will report the percentage of contents it was able to fit on the grid, as a
    # rough indicator of how close projection was to succeed. This feature is an investigation/research tool.
    # Default (false) would cause a slot's projection to terminate earlier upon failing.
    # continue-on-convergence-failure: false

  # nxt013:
    # Project CGs without internal tokens (gen2-island mode)
    # no-tokens-island: true

elrond:
  # enable builtins to load from ../etc/codegraph_builtins (enabled by default).
  # Uncomment to disable
  # enable-builtins: false
  #
  # Disable telemetry collection before optimizer is happy.
  # Also blocks loader from starting until optmizer is happy. Without using
  # "enable-handoffs" will not work, it will exit.
  # enable-telemetry-less: false
  #
  # Set to true to zero memory used as application stack
  zero-stack: false
  # To change libcall load policy, which controls how libcall implementations
  # are loaded into codegraph from disk, set this value to:
  # - `compliant-only`: Allow only IEEE-compliant implementations.
  # - `prefer-fast` (the default): Prefer selecting fast versions (no-NaNs, no-Infs,
  #   flush-to-zero, denormals-are-zero) if available, otherwise fall back to
  #   IEEE-compliant implementations.
  # libcall-load-policy: prefer-fast
  #
  # Enable different processes of the same app to share memory
  enable-app-shmem: true
  #
  # Options to handle atomic op over hwin, assert or log. Default true
  # assert-on-atomic-over-hwin: <true | false>
  preallocate-threads: 8192
  
device:
  gen2:
    # Configurable dcodes:
    # Selects the current used dcode index.
    select-current-dcode: 1
    # Add new dcodes, add entry for each dcode to add
    # custom-dcodes: []
      # - index: <dcode_idx> [1-15]
      #   col-src: <column_source> [6-22]
      #   col-cnt: <column_count> [0-3]
      #   row-src: <row_source> [6-22]
      #   row-cnt: <row_count> [0-3]
      # Example:
      # - index: 1
      #   col-src: 6
      #   col-cnt: 3
      #   row-src: 10
      #   row-cnt: 2
    # The default dcode 1 is 8x8 (i.e., full tile)
    # select-current-dcode: 1
    # custom-dcodes:
    # - index: 1
    #   col-src: 6
    #   col-cnt: 3
    #   row-src: 9
    #   row-cnt: 3
    # enable-ecore-mode: true
    # Overrides the migration copy methods if set. Useful values: "none" (memcpy), "BMT" (use bmt).
    # migrations-copy-method: ""
    # Define the number of huge pages to allocate for the runtime. Default is 1.
    # num-alloc-huge-pages: 1
    # psi-grouping-behavior:
      # Control usage of psi when mapping in page table, default true
      # enable-psi-grouping: <true/false>
      # Maximum number of tlb entries that can be created for each mapping before taking action
      # maximal-accepted-tlb-entries: <uint>
      # Action to do if maximal was exceed, default is ignore
      # action-on-tlb-entries-exceed: <ignore/warn/assert>

loader5:
  # Support memory trapping all ELF segments loaded by the dynamic loader (ld.so),
  # including sections such as: .rodata, .data, .bss
  # this allows transparently migrating these addresses to device memory
  # trap-elf-data: true
  # Support memory trapping of the main thread stack (which is allocated by the kernel).
  # Enabling this will allow the runtime to migrate the main thread stack to the device
  # memory (HBM), and back to the host
  # trap-main-thread-stack: true
  # Enable eventlog: Log for system events
  enable-eventlog: true
  # Native run (on host) when daemon connection fails
  # Configures the run when nextloader fails to conenct to nextdaemon.
  # true  - Native run (on host) after logging a warning. (default)
  # false - Log an error and crash.
  # conn-fail-native-run: true (default) | false
  # Configures the number of nextloader retries to conenct to nextdaemon
  # each retry is 1 second apart
  # conn-retries: 3

codegraph_db:
  # remove-unused: true
  # To enable or disable select_set simplification set the following
  # value to true/false. Simplification splits select_set nodes into select
  # nodes before optimization and reassembles them afterward:
  # simplify-select-sets: true
  memory-optimization:
    # enable-eliminate-barriers: false
    # enable-eliminate-barriers-force: false
    # loop-pipelining-hack: false
    classification:
      # Treat OR nodes as ADD nodes in address calculation
      # even when we can't prove they're equivalent (see mem_class.cpp)
      unsafe-or-as-add: false
    # reordering:
      # Reordering can change the order of memory accesses including atomics
      # enable: true
      # Reordering of two read only is allowed.
      # Possible values are: all, non-atomic-only, non-atomic-with-atomic
      # for safe x86 like behavior: non-atomic-only
      # read-only: all
      # Reordering of two memory accesses that do not overlap is allowed
      # Possible values are: all, non-atomic-only, non-atomic-with-atomic
      # for safe x86 like behavior: non-atomic-only
      # non-alias: all
      # UNSAFE: treating atomics and non-atomics as non-aliasing
      # unsafe-deorder-atomics-from-non-atomics: true
      # UNSAFE: treat pointers from different stack frames as non-aliasing
      # unsafe-deorder-distinct-stack-frames: false
    # coalescing:
      # enable: true
      # max-size-bytes: 16
      # enable-heuristic-alignment-optimization: false
      # Experimental heuristic to prevent split memory transactions during
      # iteration of struct arrays with sizes that do not align well with the
      # bin size (in gen1, 128bits).
      # enable-struct-array-size-heuristic: false
      # Alignment requirement setting when coalescing.
      # Given a potential merge x <- (m[0], ..., m[n-1])
      # static: coalesce is allowed only if x.alignment >= x.size
      # none: standard heuristics, disregards alignment
      # no-conflicts:
      # If there exists i such that m[i] has non-trivial alignment info,
      # that is, m[i].alignment > m[i].size,
      # then it behaves as "static". Otherwise, it behaves like "none".
      # align-req: no-conflicts
  feeder-optimization:
    slots-per-thread: 512
# Modern (elrond_core)
optimizer-pi:
  # Optimizer will skip mills which are not from those functions, ignored if empty.
  # format: Muid_Zuid or debug name
  # millable-functions: []
  # Optimizer will skip mills which are from those functions.
  # (These functions can still be inlined into other mills.)
  # format: Muid_Zuid or debug name
  # unmillable-functions: []
  # Filter out a specified mill after optimization.
  # Format: "<func_name>: <bbg_id>" (the quote marks are mandatory)
  # example:
  #   blacklist-mills:
  #   - "main: 33"
  #   - "step_10: 9"
  # blacklist-mills: []
  # if projection fails on a loop or on an epilogue, downgrade the entire loop
  # with all of its epilogues and not just the failed compute graphs
  #downgrade-entire-loop-on-failure: true
  # if projection fails on a loop and it has a closer parent loop, downgrade
  # the parent loop as well
  #prevent-closer-parent-loops: true
  # false - The optimizer will create a single grid configuration, containing all the mills at once.
  #         This is the default value.
  # true - The optimizer might create multiple grid configurations (each containing some of the mills),
  #        along with policies to schedule these configurations based on the executed code.
  # enable-mill-scheduling: true
  # Store optimization artifacts in a non-temporary directory. When running with nsopt, this field has
  # no effect, since the artifacts are always saved to the output path.
  # artifact-directory: ""
  # Possible values: none, device, device-no-grid, hwsim, hwsim-no-grid, swsim-all-domains, swsim-host-only.
  # execution-profile: device
  #loop-flattening:
    #enabled: false
    # total number of MEPs in outer head and epilogue
    #memory-access-threshold: 0
    # total number of LEs in outer head and epilogue
    #compute-nodes-threshold: 200
  #discovery:
    # The optimizer has four telemetry collection modes:
    # - "none": Skips the telemetry collection phase.
    # - "complete": Collects telemetry for the entire application runtime.
    # - "region": Collects telemetry during the first execution of a telemetry region marked using the
    #   nsapi_telem_region_enter() and nsapi_telem_region_exit() functions.
    # The default is "complete" telemetry collection.
    # telemetry-collection: none | complete | region

    # Only import functions that have been called at least this many times.
    # import-threshold: 1

  #plan:
    # Both of the following must hold:
    # 1. Keep edge e only if load(e) > edge-threshold-minimum.
    # 2. Keep highest load edge e_hi.
    #    Keep edge e only if load(e) > load(e_hi) / edge-threshold-factor.
    # The connected subgraphs that are left are mill candidates.
    # edge-threshold-minimum: 100000
    # edge-threshold-factor: 4096

    # Both of the following must hold:
    # 1. Keep mill m only if load(m) > mill-threshold-minimum.
    #    load(m) on a mill equals the edge with the highest load in the mill.
    # 2. Keep highest load mill m_hi.
    #    Keep mill m only if load(m) > load(m_hi) / mill-threshold-factor.
    # mill-threshold-minimum: 100000
    # mill-threshold-factor: 4096

    # Keep mill m with only if mill-iteration-count(m) > mill-iter-threshold.
    # Example: nested loops, outer loop has iters = 3, inner loop has iters = 4.
    #          The mill iteration count is 3 x 4 = 12.
    # mill-iter-threshold: 1000.0

    # Neighbor prioritization factor - how much more do we want to prioritize the neighbors of a projected BBG
    # (the priority of the neighboring BBGs will be multiplied by this coefficient)
    #neighbor-prioritization-factor: 1.0
    # This option determines the stability of the BBG prioritization process.
    # The value represents the minimum % difference in load between two BBGs that will be considered
    # "a different prioritization group" when determining their projection order.
    # The priority within a particular prioritization group is determined by stable, load-independent criteria.
    # A higher value (up to UINT64_MAX) results in a more stable prioritization, while a value of 0 prioritizes based solely on load.
    #prioritization-stability-factor: 15

  # Uncomment to disable break commutative:
  # enable-break-commutative: false
  # Uncomment to disable inlining order calls:
  # enable-inline-order-call: false

  # If enabled, the optimizer will not filter parallel functions that call RPCs.
  # RPCs from device threads are unsupported, however, if it is guaranteed
  # that the RPC will not be called this flag can allow scheduling them.
  # allow-rpc-in-parallel: false

  # exploration:
    # Merge epilogues backward into predecessors to spare compute graphs for projection.
    # merge-epilogue-backward:
      #enabled: true
      # Maximal number of MEPs in epilogue to merge
      #memory-access-threshold: 0
      # Maximal number of LEs in epilogue to merge
      #compute-nodes-threshold: 200

  mlc:
    enable-fork-on-grid: true
    # Optimization pipeline to run <[optimizer, telemetry-less]>.
    #   optimizer:      Telemetry based scheduling and optimizations.
    #   telemetry-less: Scheduling and optimizations based on static information only.
    # pipeline: optimizer
    # A flag to enable passes that perform optional transformations, such as
    # control flow and memory optimizations. All passes required to generate
    # a device executable, such as inlining and scheduling, will run regardless
    # of this flag.
    # enable-optional-passes: true
    # A flag to enable grid invariants at fork points.
    # enable-grid-invariants: true
    # A lower bound for the call site branch count to enable inlining
    # (default: 1).
    # inlining-threshold: 1
    # A threshold for the minimum number of iterations a loop must have
    # to be considered for acceleration (default: 1000000).
    acceleration-threshold: 1000
    # A threshold for the minimum ratio between the maximum loop
    # execution count of an accelerated loop nest and the execution
    # count of its injection loop or function, ensuring thread
    # injections don't bottleneck the accelerated loop nest (default: UINT64_MAX).
    # injection-threshold: 100000
    # A threshold for the minimum ratio between the parent loop execution
    # count and the entry/exit count of a nested branch or loop, for the
    # latter to be considered unlikely (default: UINT64_MAX).
    # unlikely-threshold: 100000
    # A multiplier used to weigh the cost of cheap arithmetic instructions
    # when executed on the host.
    # host-cheap-compute-cost: 1
    # A multiplier used to weigh the cost of expensive instructions when
    # executed on the host.
    # host-expensive-compute-cost: 10
    # A multiplier used to weigh the cost of memory accesses on host.
    # host-memory-access-cost: 200
    # A multiplier used to weigh the cost of cheap arithmetic instructions
    # when executed on auxiliary.
    # auxiliary-cheap-compute-cost: 10
    # A multiplier used to weigh the cost of expensive instructions when
    # executed on auxiliary.
    # auxiliary-expensive-compute-cost: 100
    # A multiplier used to weigh the cost of memory accesses on auxiliary.
    # auxiliary-memory-access-cost: 50
    # A multiplier used to weigh the round-trip cost of exiting to the host
    # and re-entering the device.
    # host-device-synchronization-cost: 1000000
    # A multiplier used to weigh the area utilization of expensive instructions
    # relative to cheap instructions when placed on the grid.
    # grid-expensive-compute-cost-factor: 16
    # A threshold for the amount of extra grid computation a compiler pass like
    # unrolling can introduce, measured in the number of cheap compute
    # instructions. Each cheap operation counts as one unit of extra
    # computation, while expensive operations are weighted according to the
    # user-defined cost factor.
    # grid-extra-compute-threshold: 256
    # A factor to scale the auxiliary cost before comparing them to the
    # host cost.
    # auxiliary-cost-tolarance: 0.9
    # A flag to enable parallel region ancestor scheduling, which may hand off
    # parent loops or functions to the auxiliary.
    # schedule-fork-point-ancestors: true
    # schedule-microtask-to-grid: force-on
  nxt009:
    numa-regions-per-device: 1
    numa-regions-width: 1
    numa-regions-height: 1
    # Target chip ID.
    # chip-id: 0
    # FIXME: Move numa-regions-* to a new config category. It takes effect
    # globally before we even enter the optimizer.
    # Specify number of regions to divide across grid. Fails if there is not
    # enough room on the SL for that many regions per device.
    # If set to 0, it will use as many regions as possible.
    # FIXME (SW-6829): Change the default from 1 to 0 once shown safe to do so.
    # numa-regions-per-device: 1
    # Width of each region, in tiles. All regions are identically shaped rectangles.
    # If set to 0, the width will be determined from the silicon layout.
    # numa-regions-width: 0
    # Height of each region, in tiles. All regions are identically shaped rectangles.
    # If set to 0, the height will be determined from the silicon layout.
    # numa-regions-height: 0
    # Caps the amount of "ecore workers" used.
    # If set to 0, the system will use as many workers as there are IQs and device ecores
    # available in the SL.
    # max-ecore-workers-per-region: 1
    # Level of generated code eventlogs, values are: none, team, thread, trace.
    # event-log-level: none
    # Whether to drop telemetry when queues are at capacity, or not.
    # lossless-telemetry: true

  # Uncomment to enable the OMP target mode. This mode is experimental and will
  # only consider target regions to run on the device.
  # enable-omp-target: true

  # The size of the thread pool that is used to parallelize parts of the
  # optimizer.
  # If the value is set to 0 (the default), the pool uses as many threads as
  # there are cores available.
  # thread-pool-size: 0

# Original (report_tool)
report_tool: {}
